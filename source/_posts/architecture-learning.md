---
title: Architecture Learning(WIP)
date: 2024-03-08 14:55:28
tags:
    - architecture
categories: 
    - general
lang: zh-CN
---

## 前言

在过去，我曾将架构视为一门纯粹的技术领域。然而，随着我亲自深入服务器管理和服务搭建的实践，我逐渐领悟到架构其实是一套旨在解决关键问题的方法论——即**如何在资源有限的条件下寻找最优解**。这让我认识到，我所学习的一切，都应该围绕这一核心问题展开。

架构的思考可以从两个主要方面入手：

- **评估：** 对项目进行全面分析，确定完成目标所需的资源量，以及我们能够调动的资源量。在现有资源限制下，是否有可能实现目标？能否用更少的资源达到同样的效果？

- **设计：** 评估完成后，基于可用资源进行架构设计，这可能涉及到服务器资源配置、研发资源分配、团队协作、技术实现选型等多个层面。

作为架构师，不可忽视评估与设计这两个环节。尤其是评估阶段，它经常被忽略。有时候，我们可能轻易地认为某个目标不可实现，但应反复探索是否存在其他路径，通过更少的资源或在可接受的牺牲条件下实现目标。

我倾向于从更宏观的角度去思考架构问题。因为我们面临的挑战不仅仅是技术问题。从项目构建到团队合作、从技术选型到项目管理，我希望能够培养出一种思维方式，而不仅仅是掌握具体的实现工具。

接下来，我将基于《亿级流量：网站架构核心技术》一书，深化我的架构知识。我会结合书中内容和个人思考记录笔记。这本书是一位前领导推荐的架构入门读物，我期待它能帮助我达到新的高度。

另外这是一本技术上有点陈旧的书了，比如后面示例中提到的Hystrix早就已经不再维护了，因此相关的架构示例不会跟书上一致，后续会结合一些新的示例另行叙述。

## 原则

### 墨菲定律、康威定律和二八定律

在构建系统架构时，吸纳和运用跨领域内的核心原则不仅能提高系统的健壮性、效率和可维护性，而且能够指引我们避免常见陷阱。三个广泛认知的原则——墨菲定律、康威定律和二八定律来为我们提供通用的策略和思路

#### 墨菲定律

墨菲定律的精髓在于 **“凡是可能出错的事，总会出错”** 。这个看似悲观的原则其实是一种强大的风险管理工具，提醒我们在设计时预见最坏的情况，并提前准备应对策略。

**应用策略：**

- **设计冗余**: 系统设计时考虑到可能的失败点，并提前设计好备份或冗余机制，以保证系统在组件失败时能够持续运作。
- **容错机制**: 实现容错机制，如异常处理、事务回滚等，确保系统在面临意外情况时能够优雅地恢复。
- **灾难恢复计划**: 制定和测试灾难恢复计划，确保在极端情况下能够迅速恢复服务。

#### 康威定律

康威定律：**“系统设计（架构）会被限制于创建这些系统的组织结构。”** 这意味着组织的沟通结构将直接影响到最终系统设计的形状。这揭示了系统设计与创造它的组织结构之间的内在联系，强调了沟通结构对架构成果的影响。

**应用策略：**

- **组织结构调整**: 根据所需的架构目标，调整团队结构，以促进更有效的沟通和合作，确保架构设计能够顺利实施。
- **跨部门合作**: 鼓励跨职能团队的合作，打破部门间的壁垒，促进更灵活和创新的系统设计。
- **架构一致性**: 维护架构决策的一致性和清晰性，确保各团队朝着共同的目标努力，减少架构腐化的风险。

#### 二八定律（帕累托原则）

帕累托原则指出 **“80%的效果来自20%的原因”** ，提示我们识别并集中资源于那些最具影响力的领域。这就跟《毛选》里面说的一样，我们应该集中最大的力量去解决更紧迫更具体的需求，这样才能更好更快的达成目标。

**应用策略：**

- **性能优化**：识别并优化那20%可能造成80%性能瓶颈的代码或组件。
- **功能开发**：优先开发那20%对用户最重要的功能，以提高用户满意度和业务价值。
- **问题解决**：针对导致大多数问题的根本原因进行解决，而非仅仅修复表面症状。

每个原则都提供了一种视角，它们帮助我们在复杂的系统设计过程中，做出更加明智的决策。通过对这三个原则的深入理解与应用，我们可以更有效地规避潜在的风险，减少团队的人效困境，同时提高系统的性能和用户满意度。

### 高并发原则

#### 为什么无状态设计容易进行水平扩展？

- **无需维护客户端状态**: 由于每个请求都是自包含的，服务器不需要跟踪或维护客户端的状态信息，这简化了数据管理和同步的需求。

- **负载均衡的简化**: 无状态应用程序更容易实现负载均衡。请求可以被任意地分配到集群中的任何服务器上，而不需要担心用户状态和数据的连续性问题。

- **弹性和可伸缩性**: 无状态架构允许系统根据需要动态添加或移除资源。这意味着应用可以根据需求轻松扩展（水平扩展）或缩减，提高资源利用效率和成本效益。

- **容错性和恢复能力**: 在无状态设计中，如果某个服务器失败，其他服务器可以无缝地接管请求，因为没有必要访问存储在故障服务器上的状态信息。

> **示例** 
> 
> 一个典型的无状态设计示例是基于HTTP协议的Web应用。HTTP本身是一个无状态协议，意味着每个请求都是独立的，不依赖于之前的请求。在这种设计下，Web服务器不会保存任何用户的状态信息（如登录状态、浏览历史等）。这种无状态的Web应用可以非常容易地通过增加更多的Web服务器来实现水平扩展，每个服务器都可以独立处理进来的请求。
> 
> 例如，一个电子商务平台的产品目录服务可以设计成无状态的。当用户浏览产品时，每个请求都包含了获取特定产品信息所需的全部信息（如产品ID）。服务器处理这个请求，返回产品的详细信息，而不需要知道用户之前的浏览历史。这样，该服务就可以通过增加更多的服务器实例来轻松扩展，每个实例都能独立响应用户请求，提高系统的吞吐量和可用性。

#### 服务化：逐步深化以满足需求

服务化是指将应用程序的不同功能模块化并作为独立服务部署的过程，它显著提升了软件的可维护性、可扩展性和复用性。随着服务化程度的增加，我们见证了从简单结构到复杂系统、从初级优化到高级治理的转变。

##### 进程内服务：基础模块化

- **概念**：功能模块以库或类的形式存在于单一应用进程中，通过直接方法调用实现服务。
- **示例**：一个Web应用中，用户认证和数据库访问等功能以不同的类或库形式实现，并在同一进程中运行。此方法简单直接，但在灵活性和扩展性方面有所不足。

##### 单机远程服务：初步独立

- **概念**：功能模块作为独立服务在单服务器上运行，通过网络协议（如HTTP、RPC）进行远程调用。
- **示例**：用户认证功能被拆分为独立服务，在同一台服务器的不同进程中运行，并通过HTTP API进行服务调用。这提高了模块独立性，但仍限于单机资源。

##### 集群手动注册服务：提升可用性

- **概念**：服务部署于多机集群，地址和端口通过中心或配置文件手动注册，以便消费者发现和调用。
- **示例**：用户认证服务在多机集群中运行，服务地址通过共享配置文件手动配置。客户端根据配置发现并调用服务，增强了服务的可用性和负载处理能力，但增加了管理复杂度。

##### 自动注册与发现服务：简化管理

- **概念**：服务启动时自动注册到服务注册中心，消费者通过查询注册中心自动发现服务地址。
- **示例**：用户认证服务启动时，自动注册其地址到如Consul或Eureka的服务注册中心。客户端应用通过查询注册中心自动获得服务地址，极大简化了服务管理和发现过程。

##### 服务的分组/隔离/路由：增加灵活性

- **概念**：基于服务版本、租户或其他属性进行服务分组或隔离，并根据请求特征智能路由。
- **示例**：用户认证服务可按版本或用户群体隔离，并通过API网关根据请求的版本号或用户属性进行路由。这提升了服务灵活性和可维护性。

##### 服务治理：确保高可用性

- **概念**：包括监控、容错、负载均衡、限流、熔断等机制，确保服务高可用性和稳定性。
- **示例**：对用户认证服务实施服务治理，实现服务健康监控、自动扩缩容、请求限流和熔断机制等。使用如Netflix Hystrix的框架支持这些功能。

通过以上递进介绍，服务架构从简单的进程内调用发展至可自管理和治理的分布式服务网络。每个阶段都增强了服务的独立性、扩展性和可靠性，同时也带来了更高的管理复杂度。设计服务架构时，应根据具体需求和资源状况，权衡不同服务化级别的适用性。

#### 消息队列：服务解耦、异步处理与性能优化

消息队列是现代软件架构中不可或缺的一部分，提供了一种高效的机制来解耦服务、实现异步处理，以及在高负载环境下平滑流量。此外，对于性能优化和故障容错方面，消息队列也发挥着重要作用。

##### 服务解耦

- **核心思想**：通过消息队列，生产者仅需将消息发送到队列，无需关心消息的具体消费者，实现了生产者与消费者的解耦。

- **应用示例**：在电商系统中，订单服务发布订单信息至消息队列，而库存和支付服务则作为独立的消费者处理相关任务，实现了服务之间的解耦。

##### 异步处理

- **核心思想**：消息队列支持将非即时的任务异步化处理，提升系统的响应速度和用户体验。

- **应用示例**：用户注册流程中，发送欢迎邮件和生成推荐列表等后续操作通过消息队列异步执行，避免阻塞主流程。

##### 流量削峰/缓冲

- **核心思想**：在流量高峰期，消息队列作为缓冲层，帮助平滑处理请求，防止后端服务过载。

- **应用示例**：秒杀活动中，购买请求首先进入消息队列，根据后端服务能力逐步处理，有效防止服务崩溃。

##### 性能优化

- **核心思想**：通过扩展消息队列的实例来分散负载，提升系统整体吞吐量。

- **应用示例**：大型活动中，部署多个消息队列副本分担消息流，每个实例处理部分消息，提高处理效率。

##### 防重和错误处理

- **核心思想**：实现消息的去重和错误处理机制，确保消息处理的准确性和可靠性。

- **应用示例**：
  
  - **防重**：利用订单ID等唯一标识符确保消息仅被处理一次。
  - **错误处理**：设置重试策略或将失败消息转入“死信队列”以便后续处理。

##### 消息队列带来的挑战

尽管消息队列带来许多优势，但其引入也增加了系统的复杂性。主要挑战包括： 

- **架构和维护复杂度**：引入消息队列意味着需要额外管理和维护此组件，包括监控、故障排除等。
- **并发和异步处理的复杂性**：高并发下的消息有序处理、异步通信中的稳定性保障等。
- **事务性和数据一致性**：分布式系统中，保持数据一致性和管理跨服务事务的复杂性。
- **重复和丢失消息处理**：设计机制处理消息的重复投递和潜在的消息丢失问题。

通过精心设计和选择适合的消息队列产品（如[RabbitMQ](https://www.rabbitmq.com/)、[Apache Kafka](https://kafka.apache.org/)、[Amazon SQS](https://aws.amazon.com/cn/sqs/)等），可以有效地克服这些挑战，充分发挥消息队列在现代分布式系统中的价值。

#### 数据异构

在现代软件开发和大数据处理领域，数据异构和数据闭环的概念是解决多源数据集成、管理和应用中的核心问题。在书中也提到的这两个概念，主要强调了如何处理来自多个数据源的数据问题，以及如何精确、稳定地处理和使用这些数据。在实际应用场景中，我们常常需要从多个服务收集数据，这些服务返回的数据可能结构不同，而且我们往往只需要每个数据源的部分数据。这不仅是一个关于数据处理的问题，更涉及到数据的整合、准确性和系统稳定性的维护。

我们常常会面临以下的挑战：

- **数据结构不一致**：不同来源的数据可能采用不同的格式和结构，如何有效地将它们整合成一致的格式是一个主要挑战。
- **数据质量和准确性**：如何确保聚合后的数据既准确又可靠，对于维护系统稳定性至关重要。
- **系统可维护性**：随着数据源的增加和数据量的膨胀，如何保持系统的可维护性和扩展性成为一项挑战。

我们可以通过下面的策略来解决：

- **中间层聚合**：在后端或中间层实施数据聚合逻辑，将来自不同源的数据标准化并聚合成统一格式，再提供给前端消费。这种方法有助于简化前端逻辑，减少前端处理数据的复杂度。
- **ETL流程**：通过ETL（提取、转换、加载）工具和流程，对数据进行提取、清洗、转换和加载，确保数据的一致性和准确性。ETL过程是大数据处理中常用的技术，有助于处理和整合异构数据源。

前端一样会面对数据处理的问题，我们通常会通过状态管理工具（如Redux、Pinia）和代码逻辑来保证数据同步与一致性问题。通常，我个人还是更推荐把复杂数据的处理放到后端，前端更多的把注意力集中在数据的使用上。

#### 缓存技术：加速应用性能和降低延迟

缓存技术通常是优化应用性能、减少延迟和降低对原始数据源访问压力的关键方法。它通过暂存数据的副本，避免了重复的数据请求，从而加快数据检索速度并提升用户体验。下面是各种缓存策略及其实际应用场景的简单介绍。

##### 浏览器端缓存

- **核心**：缓存网页的静态资源，如HTML、CSS、JavaScript和图像，减少服务器请求。
- **实践**：利用HTTP头部`Cache-Control`和`Expires`指令控制资源缓存。例如，`Cache-Control: max-age=3600`告诉浏览器一个小时内不需重新请求该CSS文件。

##### 客户端缓存

- **核心**：移动或桌面应用内缓存数据，以减少网络请求并提速，另外还可以提供离线的客户端操作作为兜底方案，比如缓存首屏资源。
- **实践**：在大型促销活动前，将静态资源预加载到客户端，减轻活动启动时服务器的负担。这通常涉及资源版本控制和预加载等技术。

##### [CDN](https://zh.wikipedia.org/zh-cn/%E5%85%A7%E5%AE%B9%E5%82%B3%E9%81%9E%E7%B6%B2%E8%B7%AF)缓存

- **核心**：内容分发网络（CDN）在全球多地缓存内容，提高全球访问速度。
- **实践**：将静态资源部署至CDN，实现资源请求由用户最近的服务器响应，显著减少加载时间。

##### 接入层缓存

- **核心**：在架构的前端部署缓存，存储频繁访问内容。
- **实践**：
  - **配置[Nginx](https://nginx.org/en/)作为反向代理缓存**：设置Nginx来缓存动态内容的输出。例如，可以对经常访问的首页、产品列表页等进行缓存。通过Nginx的`proxy_cache`指令，你可以定义缓存的键、有效期和存储路径。
  - **使用[Varnish](https://github.com/varnishcache/varnish-cache)进行高级缓存**：Varnish是另一个强大的HTTP缓存系统，能够处理更复杂的缓存逻辑。通过VCL（Varnish Configuration Language），你可以编写规则来决定哪些请求和响应应该被缓存以及如何缓存。

##### 应用层缓存

- **核心**：应用内部缓存机制，针对特定数据或计算结果进行缓存。

- **实践**：
  
  - **内存缓存**：使用[Redis](https://redis.io/)或[Memcached](https://memcached.org/)缓存数据库查询结果或其他计算数据。
  - **应用内缓存机制**：在代码级别实现缓存，如利用HashMap缓存计算密集型数据结果。

##### 分布式缓存

- **核心**：构建分布式缓存系统，保障大规模应用的数据可扩展性和高可用性。

- **实践**：
  
  - **部署Redis或Memcached集群**：实现数据的自动分片和复制。
  - **数据一致性和分片策略**：采用一致性哈希算法等技术确保缓存系统的负载均衡和高可用。

正确利用各类缓存技术不仅能提高应用的性能，还能在高流量情况下保持系统的稳定性，显著改善最终用户的体验。每种缓存策略都有其适用场景和考虑要点，根据应用的具体需求和环境选择最合适的缓存解决方案至关重要。

除了在整体架构中采用缓存方案外，缓存技术在前端应用开发中也扮演着至关重要的角色，尤其是在提升用户交互体验方面。以实时编辑器为例，利用本地缓存暂存用户输入的内容可以显著改善在网络条件不佳时的使用体验。当用户的网络连接恢复时，应用可以在后台自动保存并同步数据，从而确保用户在编辑过程中几乎感受不到任何延迟或卡顿。这种方法不仅保证了数据的安全性和实时性，也使用户体验更加流畅和愉悦。

自然，任何缓存策略都会遇到数据一致性和有效性的挑战。因此，设计一个高效且运行良好的缓存方案变得至关重要。这不仅涉及到技术选择和实现细节，还包括对数据更新机制、失效策略和一致性保障的周到规划，以确保缓存带来的性能提升不会以牺牲数据准确性为代价。

### 高可用原则

#### 系统降级策略

在管理高流量的网站架构时，为了保持系统的高可用性和稳定性，采用降级策略是一种常见做法。这些策略使我们能够在面对极端流量或系统压力时，灵活应对，确保关键服务的持续可用。下面是几种典型的降级策略：

##### 开关集中化管理

集中化管理系统的各类开关（如功能开关、流量控制开关）能够使我们在系统状态发生变化时迅速作出反应，比如临时关闭某些功能或服务以应对突增的流量。

**示例**：想象在大促销期间，电商平台的某个后端服务（例如，推荐系统）面临极大的访问压力。通过集中管理的开关，运维团队可以迅速禁用推荐服务，减轻系统负荷，而不会影响到平台的整体运行。

##### 可降级的多级读服务

在一个拥有多级缓存架构的系统中，数据会被存储在不同的级别（例如，内存、本地缓存、分布式缓存）。可降级的多级读服务允许系统在上级缓存不可用时自动降级到下一级缓存或直接从数据库中读取数据，保障数据的持续可用性。

**示例**：在一个既使用了内存缓存也使用了分布式缓存的系统中，如果内存缓存由于某些原因（如服务器重启）不可用，系统将自动切换到分布式缓存来获取数据。如果分布式缓存同样无法访问，系统则直接从数据库中读取数据，确保服务的连续性。

##### 开关前置化

将流量控制和功能开关等控制逻辑放置于系统的最前端（如负载均衡器或API网关），这样可以在请求进入应用层之前进行必要的流量控制和功能调整，减少后端服务的负担。

**示例**：在Nginx+Tomcat的架构下，若Tomcat应用承受过大的流量压力，Nginx层可以通过流量限制或重定向某些请求到错误或降级页面，有效阻止过量流量影响Tomcat层。

###### 实现Nginx层的开关控制策略

- **流量限制（Rate Limiting）**：利用Nginx的限流指令（如`limit_req_zone`），可以基于每秒请求数量设定限流规则，对超出限制的请求进行延迟处理或返回错误响应，有效避免后端服务过载。

- **功能开关（Feature Toggle）**：通过Nginx配置中的`if`语句或逻辑判断，根据请求的URL、时间等条件动态启用或禁用特定功能，灵活控制服务的可用性。

- **维护模式**：在系统需进行维护或升级时，可快速在Nginx配置中启用维护模式，将所有请求引导至维护页面，而无需中断后端应用运行。

- **示例配置：**
  
  ```nginx
  # 指令用于定义一个限流区域，这个区域将用于存储会话状态。
  # https://nginx.org/en/docs/http/ngx_http_limit_req_module.html#limit_req
  # $binary_remote_addr 表示限流是基于每个独立IP进行的。
  # zone=mylimit:10m 创建一个名为 mylimit 的存储区域，大小为 10MB。这个存储大小决定了 Nginx 能够跟踪的并发请求数量。
  # rate=10r/s 设定允许通过的请求速率为每秒 10 个请求
  limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;
  
  server {
      listen 80;
      server_name example.com;
  
      # 维护模式开关
      set $maintenance 0; # 0 表示正常运行，1 表示维护模式
  
      if ($maintenance) {
          return 503 '<html><body>系统维护中，请稍后再试。</body></html>';
      }
  
      location / {
          # 流量限制配置
          limit_req zone=mylimit burst=20 nodelay;
  
          # 正常情况下的请求转发
          proxy_pass http://tomcat_backend;
      }
  
      # 定义一个特殊的URI用于返回"系统繁忙"页面
      location /busy {
          return 503 '<html><body>系统繁忙，请稍后再试。</body></html>';
      }
  
      # 这部分被注释的配置表示一个降级操作的示例。
      # 若取消注释，所有对根路径 / 的请求将被重写到 /busy
      # 导致所有用户都会看到“系统繁忙”的页面。
      # 这是一种简单有效的流量控制手段，用于紧急情况下快速减轻后端系统压力。
      # 当系统需要降级处理时，将所有请求重定向到/busy
      # 通过修改配置文件和重新加载Nginx来动态开启降级
      # location / {
      #     rewrite ^ /busy;
      # }
  }
  ```

我们再具体说一下nginx限流区域的问题，这时nginx通过`limit_req_zone`指令，用于跟踪和控制请求的速率。这个区域在Nginx的工作进程之间是共享的，所以限流规则可以跨多个客户端连接和请求生效，这里面存放的主要是请求的计数器或标记，它们主要用于跟踪特定的键值（如客户端的IP地址），通常不需要太大的内存。

##### 业务降级

当系统压力过大或某些服务不可用时，暂时关闭或降低某些非核心业务的服务质量，从而确保系统的核心功能能够稳定运行。

**示例**：在电商网站的大促活动期间，为了保障订单系统的平稳运行，可能会暂时关闭或降级一些辅助功能，如商品推荐、评论功能等，以优先保障订单处理等核心业务的资源需求。

#### 限流、流量切换和版本控制

- 限流是预防系统过载的重要手段，尤其针对突发或恶意流量。上面提到的Nginx的限流配置提供了一个实际操作的例子，通过控制请求速率来避免服务崩溃。

- 流量切换能够在服务出现故障时迅速响应，减少服务中断时间。下面是一些常见的方式：
  
  - **DNS切换**：通过更改DNS记录，切换到备用机房或服务入口，实现大范围的流量重定向。
  
  - **[HttpDNS](https://zhuanlan.zhihu.com/p/102839806)**：在应用程序场景下，HttpDNS允许客户端直接确定流量入口，绕过可能存在问题的本地DNS，实现更精细的流量控制。
  
  - **[LVS/HaProxy](https://zhuanlan.zhihu.com/p/71825940)**：用于在负载均衡层面切换流量，以隔离或绕过出现故障的Nginx接入层。
  
  - **Nginx切换**：在应用层面进行切换，如将流量从故障服务重定向到健康服务，保持应用的可用性。

- 良好的版本控制不仅保证了代码的可审计性和可回溯性，也支持快速回滚到稳定版本以应对新版本可能引入的问题。有效的版本管理策略和回滚机制对于快速定位问题和恢复服务至关重要。
  
  - **可审计**：确保每次发布都有明确的版本记录，便于追踪变更和定位问题。
  
  - **可回溯**：在发现新版本问题时，能够迅速回滚到上一个稳定版本，减少故障影响时间。
  
  - **回滚机制**：设计灵活可靠的回滚操作，包括自动化脚本和手动干预，以应对各种回滚场景。

## 高可用策略

### 负载均衡与反向代理

负载均衡，简单点说，就是会将用户的访问分配到不同的服务器上，以解决单服务器压力过大或是服务器出现故障等问题。外网DNS使用全局负载均衡（GSLB）来进行流量调度，内网则通过简单的轮询负载均衡或借助一些专用的中间层，如LVS、F5、HaProxy、Nginx等。对于一般应用而言用Nginx即可，它一般用于HTTP七层负载均衡，现在也已支持TCP四层负载均衡。我们在简单解释下上面提到的几个概念：

- 全局负载均衡：全局负载均衡（Global Load Balancing）是一种网络架构或技术，旨在平衡流量和请求分发到多个地理位置的服务器或数据中心。

- 二层负载均衡：是指在 OSI 模型的数据链路层（第二层）对网络流量进行负载均衡的一种技术。这通常涉及到对数据包的目标 MAC 地址进行改写，将其修改为负载均衡器与真实服务器之间的 MAC 地址。这样做的目的是让负载均衡器能够接收到流量，然后再根据负载均衡策略将流量转发给相应的真实服务器。
  
  **数据链路层的负载均衡**：
  
  - 在二层负载均衡中，负载均衡器工作在 OSI 模型的数据链路层。它通过修改数据包的目标 MAC 地址来拦截并重定向流量。
  - 负载均衡器和真实服务器共享同一个虚拟 IP 地址（VIP），因此所有传入的数据包都会被发送到负载均衡器。
  - 当负载均衡器接收到数据包后，它会根据负载均衡策略选择一个真实服务器，并将数据包的目标 MAC 地址修改为该服务器的 MAC 地址，然后将数据包发送到网络上。
  
  **改写数据包的 MAC 地址**：
  
  - 负载均衡器通过改写数据包的目标 MAC 地址来实现流量的转发。这样做的效果是，流量似乎直接发送给了真实服务器，而不是经过负载均衡器。
  - 负载均衡器与真实服务器之间需要事先建立好 ARP（地址解析协议）缓存，以确保负载均衡器能够正确识别真实服务器的 MAC 地址。
  
  **VIP 的作用**：
  
  - 虚拟 IP 地址（VIP）是负载均衡器和外部客户端通信的入口。所有传入的流量都会发送到 VIP 上。
  - 通过共享同一个 VIP，负载均衡器能够接收到所有传入的流量，并根据负载均衡策略将流量转发给真实服务器。
  
  **LVS DR 工作模式**：
  
  - LVS（Linux Virtual Server）是一个开源的负载均衡器软件，支持多种负载均衡模式，包括 Direct Routing（DR）模式。
  - 在 LVS DR 模式中，负载均衡器与真实服务器在同一个局域网内，它们共享同一个 VIP，并且负载均衡器不会修改数据包的 IP 地址。只有目标 MAC 地址会被修改为真实服务器的 MAC 地址，以实现流量的转发。

- 四层负载均衡：是指在 OSI 模型的传输层（第四层）对网络流量进行负载均衡的一种技术。这种负载均衡是根据传输层的信息，如源 IP 地址、目标 IP 地址、源端口和目标端口等，来决定如何转发数据包。四层负载均衡通常用于基于端口的负载均衡，它能够将传入的连接或数据包根据端口信息分发到不同的上游服务器。如LVS NAT模式、HaProxy等

- 七层负载均衡：是指在 OSI 模型的应用层（第七层）对网络流量进行负载均衡的一种技术。这种负载均衡是根据应用层的信息，如主机名、URL、HTTP 头部等，来决定如何转发数据包。七层负载均衡通常用于基于应用层协议的负载均衡，例如 HTTP、HTTPS、SMTP、FTP 等。常见软件如HaProxy、Nginx等

因为LVS和上游服务器必须在同一个子网，为了解决跨子网问题而又不影响负载性能，我们可以使用LVS+HaProxy(or Nginx)的集群来解决跨网和性能问题。

对于负载均衡我们通常要关心以下的内容

- 上游服务器的配置

- 负载均衡的算法

- 失败重试机制

- 服务器心跳检查

通常我们会搭配各种软件来进行更智能的负载均衡，比如OpenResty。另外Nginx除了是负载均衡还是一台[反向代理](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86)服务器。

#### upstream配置

```nginx
# 定义后端服务器，weight为此服务器的权重
upstream backend {
    server 192.168.61.1:9080 weight=1;
    server 192.168.61.1:8080 weight=2;
}

location / {
    proxy_pass http://backend;
}
```

#### 负载均衡算法

负载均衡算法是网络架构中关键组件，确保请求均匀分布于服务器，提升系统稳定性和处理能力。以下是几种常用的负载均衡算法：

##### 轮询(Round Robin)

默认的负载均衡算法（事实上是加权轮询），以轮询的方式将请求转发到上游服务器，通过配合权重配置实现基于权重的轮询。

##### 基于IP的哈希(IP Hash)

根据客户端的 IP 地址计算哈希值，然后根据哈希值将请求分配到服务器。这种方法确保来自同一客户端的请求总是被发送到同一服务器（前提是服务器列表保持不变）。

```nginx
upstream backend {
    ip_hash;
    server 192.168.61.1:9080 weight=1;
    server 192.168.61.1:8080 weight=2;
}
```

##### 最少连接数(Least Connections)

将新请求分配给当前连接数最少的服务器，优化资源利用率，适用于会话长度不一的场景。如果有多个服务器的连接数相同，则按照轮询策略选择服务器。

##### 哈希算法 (Hash Algorithm)

通过哈希函数，基于特定键值（如URL、请求参数）计算哈希值，根据结果选择服务器。包括基本哈希和一致性哈希算法。

- **哈希算法：** 在负载均衡的上下文中，哈希算法通常用于确定将传入的请求分配给哪个后端服务器。基本的哈希算法包括根据某种键值（如客户端IP地址、请求的URL等）进行哈希计算，然后根据哈希值选择后端服务器。哈希算法的目标是尽量均匀分配负载，同时保持特定请求的会话亲和性（Session Affinity）。
  
  - 常见操作步骤：
    
    1. 选择一个键值（如客户端IP、请求的URL）作为输入。
    
    2. 使用哈希函数对该键值进行哈希计算，产生一个哈希值。
    
    3. 将哈希值映射到后端服务器。常见的方法是哈希值对服务器数量取模（例如，哈希值 % 服务器数量）。
  
  . 缺陷：如果后端服务器的数量变化（添加或删除服务器），大部分请求都可能被映射到新的服务器，这会导致缓存失效和会话不连贯等问题。
  
  ```nginx
  upstream backend {
      hash $url;
      server 192.168.61.1:9080 weight=1;
      server 192.168.61.1:8080 weight=2;
  }
  ```

. **一致性哈希算法：** 一致性哈希算法是为了解决传统哈希算法在服务器列表变动时导致的大量重新映射问题而设计的。一致性哈希将哈希值空间组织成一个虚拟的圆环（或哈希环），并将服务器映射到这个环上的某个点上。

  . 常见操作步骤：

    1. 将哈希空间组织成一个虚拟的圆环。
    
    2. 根据服务器的标识（如IP地址）计算哈希值，并将每个服务器放置在哈希环上的相应位置。
    
    3. 对于每个请求，根据请求的键值计算哈希值，然后在哈希环上顺时针寻找到第一个遇到的服务器作为目标服务器。

  . 优点：

    - **服务器增减时的高效性**：当增加或删除服务器时，只有一个很小部分的请求会被重新映射到其他服务器，大大减少了缓存失效和会话不连贯的问题。
    - **负载均衡**：一致性哈希尽量保证请求均匀分布到各个服务器。

```nginx
upstream backend {
    hash $consistent_key consistent;
    server 192.168.61.1:9080 weight=1;
    server 192.168.61.1:8080 weight=2;
}


location / {
    set $consistent_key $xxx
    if ($consistent_key = "") {
        set $consistent_key $request_uri;
    }
}
```

实际操作中，我们可以使用OpenResty+Lua+nginx来灵活的配置一致性哈希的key，书中代码如下

```nginx
location / {
    set_by_lua_file $consistent_key "lua_balancing.lua";
}
```

```lua
-- lua_balancing.lua
-- 从参数中获取xxx参数
local args = ngx.req.get_uri_args()
local consistent_key = args.xxx

if not consistent_key or consistent_key == '' then
    consistent_key = ngx.var.request_uri
end

-- 尝试从缓存中获取预定义的实例
local value = balancing_cache:get(consistent_key)
if not value then
    -- 如果不存在，则设置一个key为1，并且时间为60s
    success, err = balancing_cache:set(consistent_key, 1, 60)
else
    -- 存在则递增key值
    newval, err = balancing_cache:incr(consistent_key, 1)
end

-- 当递增的key大于某个阈值的时候则递增到一个新的服务器
if newval > 5000 then
    consistent_key = consistent_key .. '_' .. newval
end
```

#### 长连接

我们在网络请求中使用长连接的目的是为了达到下面的目的：

- **减少连接建立和关闭的开销**：每次建立新的 TCP 连接都涉及到三次握手过程，这会增加延迟和处理开销。通过重用已经建立的连接，可以减少这些额外的开销。

- **提高请求的处理速度**：因为省去了重复建立连接的时间，所以长连接可以提高请求的处理速度，尤其是在高流量的情况下。

- **减少并发连接数**：长连接可以减少服务器需要同时处理的并发连接总数，这对于保持服务器的性能和稳定性非常重要。

但它也有其优劣势，所以在使用的时候需要好好的考虑和规划

##### 优势

- **性能提升**：避免频繁的连接建立和关闭，降低了 TCP 握手的延迟，对性能是一个直接的提升。

- **减轻服务器负载**：长连接减少了服务器必须处理的网络连接的数量，从而减轻了服务器的负载。

- **提高吞吐量**：由于减少了连接建立的时间，长连接可以在同一时间内处理更多的请求，从而提高吞吐量。

##### 劣势

- **资源占用**：即使在没有数据传输的时候，长连接也会保持打开状态，这会占用服务器的资源，如文件描述符、内存等。

- **不活跃连接**：如果客户端忘记关闭连接，或者由于网络问题导致连接没有正常关闭，可能会导致服务器上积累很多不活跃的连接，这些都是无谓的资源占用。

- **潜在的资源耗尽问题**：在高负载环境下，长连接可能会导致服务器资源耗尽，尤其是在短时间内有大量连接建立时。

##### nginx配置示例

```nginx
upstream backend {
    server ipaddr_1 weight=1;
    server ipaddr_2 weight=2 backup;
    keepalive 100;
}

location / {
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    # proxy_set_header Connection "Keep-Alive" if http_version == 1.0
    proxy_pass http://backend;
}
```

- 注意上游服务器也需要开启长连接支持

##### nginx[长连接的获取和释放实现代码](https://trac.nginx.org/nginx/browser/nginx/src/http/modules/ngx_http_upstream_keepalive_module.c)解析

- **ngx_http_upstream_get_keepalive_peer**

```c
// 从长连接池中获取一个已经建立的连接
static ngx_int_t
ngx_http_upstream_get_keepalive_peer(ngx_peer_connection_t *pc, void *data)
{
    ngx_http_upstream_keepalive_peer_data_t  *kp = data;
    ngx_http_upstream_keepalive_cache_t      *item;

    ngx_int_t          rc;
    ngx_queue_t       *q, *cache;
    ngx_connection_t  *c;

    ngx_log_debug0(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                   "get keepalive peer");

    /* ask balancer */
    // 尝试获取一个新的上游连接
    rc = kp->original_get_peer(pc, kp->data);

    if (rc != NGX_OK) {
        return rc;
    }

    /* search cache for suitable connection */
    // 获取长连接缓存的引用
    cache = &kp->conf->cache;
    // 开始遍历缓存队列并寻找一个可用的连接
    for (q = ngx_queue_head(cache);
         q != ngx_queue_sentinel(cache);
         q = ngx_queue_next(q))
    {
        item = ngx_queue_data(q, ngx_http_upstream_keepalive_cache_t, queue);
        c = item->connection;
        // 检查缓存项的地址是否和当前请求的地址匹配
        if (ngx_memn2cmp((u_char *) &item->sockaddr, (u_char *) pc->sockaddr,
                         item->socklen, pc->socklen)
            == 0)
        {
            ngx_queue_remove(q);
            ngx_queue_insert_head(&kp->conf->free, q);
            // 找到就进入到found处理
            goto found;
        }
    }

    return NGX_OK;

found:

    ngx_log_debug1(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                   "get keepalive peer: using connection %p", c);
    // 设置为非空闲
    c->idle = 0;
    c->sent = 0;
    c->data = NULL;
    c->log = pc->log;
    c->read->log = pc->log;
    c->write->log = pc->log;
    c->pool->log = pc->log;

    if (c->read->timer_set) {
        ngx_del_timer(c->read);
    }
    // 赋值
    pc->connection = c;
    pc->cached = 1;
    // 表示获取到了一个长连接
    return NGX_DONE;
}
```

- **ngx_http_upstream_free_keepalive_peer**

```c
// 将不再使用的长连接放回到连接池中
static void
ngx_http_upstream_free_keepalive_peer(ngx_peer_connection_t *pc, void *data,
    ngx_uint_t state)
{
    ngx_http_upstream_keepalive_peer_data_t  *kp = data;
    ngx_http_upstream_keepalive_cache_t      *item;

    ngx_queue_t          *q;
    ngx_connection_t     *c;
    ngx_http_upstream_t  *u;

    ngx_log_debug0(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                   "free keepalive peer");

    /* cache valid connections */

    u = kp->upstream;
    c = pc->connection;
    // 检查连接状态，有错误或超时则进入到invalid进行清理
    if (state & NGX_PEER_FAILED
        || c == NULL
        || c->read->eof
        || c->read->error
        || c->read->timedout
        || c->write->error
        || c->write->timedout)
    {
        goto invalid;
    }

    if (c->requests >= kp->conf->requests) {
        goto invalid;
    }

    if (ngx_current_msec - c->start_time > kp->conf->time) {
        goto invalid;
    }

    if (!u->keepalive) {
        goto invalid;
    }

    if (!u->request_body_sent) {
        goto invalid;
    }

    if (ngx_terminate || ngx_exiting) {
        goto invalid;
    }

    if (ngx_handle_read_event(c->read, 0) != NGX_OK) {
        goto invalid;
    }

    ngx_log_debug1(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                   "free keepalive peer: saving connection %p", c);

    /* 
     * 在释放长连接时，检查空闲队列是否为空的步骤是为了管理和重用长连接池中的连接。
     * 长连接池中维护着两个队列：一个用于存放当前空闲的连接（称为“空闲队列”），
     * 另一个用于存放所有连接（称为“缓存队列”）。
     * 当有连接不再被使用时，我们会尝试将它放回到空闲队列中，以便它可以被后续的请求重用。
     * 如果空闲队列不为空，说明还有现成的连接可以被重新利用，
     * 我们就可以直接将刚释放的连接放入空闲队列中。
     * 然而，如果空闲队列为空，那意味着当前没有多余的连接可以直接用于新的请求，
     * 这时，我们可能需要从缓存队列中移除一些连接以维持池中连接数量的平衡。
     */

    // 检查长连接的空闲队伍是否为空
    if (ngx_queue_empty(&kp->conf->free)) {
        // 为空表示当前没有可供重用的连接  
        // 因此需要从缓存队列中关闭清空一项（下面的close操作），把连接让出来
        // 因为淘汰最不常用的，按照先进先出，取出的是队列末尾的连接（最早放入的）
        q = ngx_queue_last(&kp->conf->cache);
        ngx_queue_remove(q);

        item = ngx_queue_data(q, ngx_http_upstream_keepalive_cache_t, queue);

        ngx_http_upstream_keepalive_close(item->connection);

    } else {
        // 不为空则表示还有多余可以使用的节点，则直接移除一个节点放入
        q = ngx_queue_head(&kp->conf->free);
        ngx_queue_remove(q);

        item = ngx_queue_data(q, ngx_http_upstream_keepalive_cache_t, queue);
    }

    ngx_queue_insert_head(&kp->conf->cache, q);

    item->connection = c;

    pc->connection = NULL;

    c->read->delayed = 0;
    ngx_add_timer(c->read, kp->conf->timeout);

    if (c->write->timer_set) {
        ngx_del_timer(c->write);
    }

    c->write->handler = ngx_http_upstream_keepalive_dummy_handler;
    c->read->handler = ngx_http_upstream_keepalive_close_handler;

    c->data = item;
    c->idle = 1;
    c->log = ngx_cycle->log;
    c->read->log = ngx_cycle->log;
    c->write->log = ngx_cycle->log;
    c->pool->log = ngx_cycle->log;

    item->socklen = pc->socklen;
    ngx_memcpy(&item->sockaddr, pc->sockaddr, pc->socklen);

    if (c->read->ready) {
        ngx_http_upstream_keepalive_close_handler(c->read);
    }

invalid:

    kp->original_free_peer(pc, kp->data, state);
}
```

- 注意，上面的实现其实有两个队列，分别是缓存队列和空闲队列
  
  - **缓存队列（cache queue）**：保存所有建立并可能重用的长连接。当一个连接不再被当前请求使用时，它会被放回缓存队列以便后续请求可以快速重用。
  
  - **空闲队列（free queue）**：当一个连接被放回缓存队列时，其对应的节点会被移动到空闲队列。空闲队列是一个节点池，它保存了当前没有与任何实际 TCP 连接关联的节点。这些节点可以用来保存新释放的或新建立的连接。

#### 动态负载均衡

像上面看到里的例子中配置的负载均衡不是动态的，即每次upstream列表有变更，都需要在服务器上手动变更。因此我们为了使管理负载均衡更轻松，就需要一种服务发现、注册的服务，简单点说就是监听服务器的各种变化，自动注册和重启Nginx。

### 隔离

系统隔离是一种重要的高可用性策略，它能够在服务出现故障时局限问题范围，确保问题服务的不可用性不会影响到整个系统的其它部分。通过有效的资源隔离，可以降低服务间的资源竞争，增强服务的独立性和可用性。

隔离策略包括但不限于线程隔离、进程隔离、集群隔离、机房隔离、读写隔离、快慢隔离、动静资源隔离及爬虫隔离等。简而言之，隔离策略通过分割系统的不同部分，确保它们独立运行而互不影响，从而保证系统的整体稳定性和高可用性。

#### 动静隔离

动静隔离是将静态资源与动态内容分离的过程，常见的做法是将静态资源部署在CDN上，以减轻后端服务器的负担。

#### 爬虫隔离

面对大量爬虫访问可能对服务造成的冲击，一般通过限流和隔离来应对。例如，在负载均衡层面，特定的爬虫请求可以被路由至独立的服务器集群，以保障正常用户流量的稳定性。一个简单的Nginx配置示例如下：

```nginx
set $flag 0;
if ($http_user_agent ~* "spider") {
    set $flag "1";
}
if ($flag = "0") {
    ……
}
if ($flag = "1") {
    ……
}
```

实际场景中我们利用OpenResty进行更细致的用户代理过滤和恶意IP过滤，以实现更精准的流量控制。在过滤恶意ip的时候，我们通常会考虑IP + Cookie的方式来精确定位。

#### 资源隔离

资源隔离关注于如何高效分配和使用硬件资源，如CPU、硬盘等。例如，在数据同步密集的场景中，可以预留特定硬盘资源以优化性能。对于大量写操作的容器，分配独立硬盘以隔离I/O操作也是常见的做法。

针对CPU资源，可以通过特定的系统工具和配置为不同服务或实例分配CPU资源，如使用`cat/proc/interrupts`检查并通过`/proc/irq/N/smp_affinity`手动设置CPU的中断绑定，或启用系统优化功能以提升整体性能。

### 限流策略

限流是一种关键的系统保护机制，旨在控制请求的流量和频率，以防止过载。通过在特定条件下（如时间段、访问频率）对请求进行限制，限流能够确保系统的稳定性和响应性。实现有效的限流策略，需要细致评估并设计，以避免对正常业务流量的负面影响。

#### 常用限流算法

##### 令牌桶算法 (Token Bucket)

令牌桶算法通过维护一个容纳固定数量令牌的桶来控制流量，每个令牌代表着可以处理的一个请求或数据包。当请求到达时，只有在取得令牌的情况下才能被处理，否则请求将被阻塞或丢弃，以此达到流量控制的目的。

**核心要素：**

- **令牌桶容量**：桶中可以存放令牌的最大数量。
- **令牌填充速率**：系统以固定速率向桶中添加令牌。
- **请求处理规则**：每个请求需要消耗一定数量的令牌才能被处理。

##### 漏桶算法 (Leaky Bucket)

漏桶算法通过模拟水桶漏水的过程来实现流量控制。数据包或请求被视为流入桶中的水，以固定速率从桶中流出。如果桶满时还有数据包到达，则新到达的数据包将被丢弃。

**核心要素：**

- **桶容量**：桶可以存放数据包的最大数量。
- **流出速率**：数据包从桶中流出的固定速率。
- **数据包处理规则**：桶满时，新到达的数据包被丢弃。

##### 其他算法

- **计数器算法**：计数器算法是一种简单直观的限流算法，它通过统计一定时间窗口内的请求数量来进行限流。当请求到达时，系统会检查当前请求数是否超过了预设的阈值，如果超过则进行限流。计数器算法简单易实现，但对于突发流量的处理能力较差。

- **滑动窗口算法**：滑动窗口算法在计数器算法基础上增加了时间的动态性，将时间窗口分为多个小窗口，并根据流量变化动态调整窗口的大小和滑动的速率，以实现更精细的流量控制。

#### 分布式限流

##### 利用Redis+Lua实现简单的限流策略

- **Lua脚本（limit.lua）**

```lua
-- 获取传递给 Lua 脚本的第一个键参数，这里 key 用于标识特定的限流对象
local key = KEYS[1]
-- 获取传递给 Lua 脚本的第一个值参数，并将其转换为数字。
local limit = tonumber(ARGV[1])
-- 尝试从 Redis 中获取当前的计数值，如果不存在，则默认为 "0"。
local current = tonumber(redis.call('get', key) or "0")
-- 如果当前计数加 1 超过了设定的限制 (limit)，则返回 0，表示请求被限流
if current + 1 > limit then
    return 0
else 
    -- 如果没有超过限制，使用 INCRBY 命令将 key 对应的计数值加 1
    redis.call("INCRBY", key, "1")
    --[[ 
    为这个 key 设置一个过期时间，这里是 2 秒。
    这意味着每个计数值的有效期为 2 秒，在这个时间窗口内，
    如果请求次数超过 limit，则会被限流 
    --]]
    redis.call("expire", key, "2")
    return 1
end
```

- **Java调用示例**

```java
public static boolean acquire() throws Exception {
    string luaScript = Files.toString(new File("limit.lua", Charset.defaultCharset());
    // 创建一个 Jedis 实例，用于连接到 Redis 服务器。
    Jedis jedis = new Jedis("ipPath", port);
    // 生成一个基于当前时间的 key，用于标识每次请求。
    String key = "ip:" + System.currentTimeMillis();
    String limit = "3";
    return (Long)jedis.eval(
                luaScript,
                Lists.newArrayList(key),
                Lists.newArrayList(limit)
            ) == 1;
}
```

此方法使用Redis进行计数，并利用Lua脚本的原子性操作确保限流逻辑的准确性。通过为每个限流键设置过期时间，可以自动重置计数，从而实现在指定时间窗口内的请求控制。此示例中，将请求限制在每2秒内最多3次请求，为系统提供了一种简单而有效的流量控制机制。

##### 利用Nginx+Lua实现简单的限流策略

- **Lua脚本（limit.lua）**

```lua
-- 加载 OpenResty 的 resty.lock 库，用于在限流过程中同步不同请求之间的访问。
local locks = require "resty.lock"
local function acquire()
    -- 创建一个新的锁对象，使用 locks 字典存储锁的状态。
    local lock =locks:new("locks")
    local elapsed, err = lock:lock("limit_key")
    -- 获取 Nginx 配置中声明的共享内存字典 limit_counter，用于跟踪 IP 地址对应的请求计数。
    local limit_counter = ngx.shared.limit_counter
    -- 使用当前时间秒作为键，每个键代表一秒钟内的请求计数。    
    local key = "ip:" .. os.time()
    local limit = 5
    local current = limit_counter:get(key)
    
    if current ~= nil and current + 1 > limit then
        lock:unlock()
        return 0
    end
    if current == nil then
        limit_counter:set(key, 1, 1)
    else 
        limit_counter:incr(key, 1)
    end
    lock:unlock()
    return 1
end
```

- **Nginx配置**

```nginx
http {
    ……
    # locks：分配了 10m 的空间给 resty.lock 使用，用于存储锁的状态信息。
    lua_shared_dict locks 10m;
    # limit_count：同样分配了 10m 的空间，用于存储每个 IP 每秒请求的计数。
    lua_shared_dict limit_count 10m;
}
```

#### 接入层限流

对于基于Nginx接入层进行限流，通常使用Nginx自带的两个模块：`ngx_http_limit_conn_module`和`ngx_http_limit_req_module`。对于更复杂的场景还可以使用OpenResty的`lua_resty_limit_traffic`

`ngx_http_limit_conn_module`是对某个key对应的总的网络连接数进行限流；`ngx_http_limit_req_module`是对某个key对应的请求的平均速率进行限流，一般有两种用法平滑模式(delay)和允许突发模式(nodelay)。

##### ngx_http_limit_conn_module

```nginx
http {
    # $binary_remote_addr 表示客户端 IP 的二进制格式，$server_name 表示服务器名
    limit_conn_zone $binary_remote_addr/$server_name zone=addr:10m;
    limit_conn_log_level error;
    limit_conn_status 503;
    # ...
    server {
        # ...
        location /limit {
            limit_conn addr 1;    
        }
    }
}
```

##### ngx_http_limit_req_module

```nginx
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
    limit_conn_log_level error;
    limit_conn_status 503;
    # ...
    server {
        # ...
        location /limit {
            limit_req zone=one brust=5 nodelay;    
        }
    }
}
```

#### 节流和防抖

节流（Throttling）和防抖（Debouncing）是两种常见的控制函数执行频率的技术，用于优化事件处理的性能和响应性。虽然它们的目标相似，即减少在短时间内触发的事件处理次数，但实现逻辑和应用场景有所不同。

##### 节流（Throttling）

节流策略确保函数在固定的时间间隔内最多只被执行一次，无论触发事件有多频繁。这意味着在这个时间段内的第一个事件会立即执行（或者是最后一个），而之后触发的事件将被忽略，直到下一个时间间隔开始。

- **应用场景**：适用于需要按固定频率更新的情况，如滚动事件的监听、浏览器窗口大小调整等。

##### 防抖（Debouncing）

去抖动策略将事件连续触发时的多次执行合并为一次执行。具体来说，只有在最后一次事件触发后的一定延迟内没有新的事件被触发，函数才会执行。如果在延迟时间内再次触发事件，计时器会被重置，直到延迟期结束无新的触发，函数才会执行。

- **应用场景**：适用于处理成本较高的操作，如实时搜索和自动完成功能，在用户停止输入后才执行搜索。



### 降级策略

服务降级是在资源紧张或系统压力过大时，有意识地暂时减少服务功能或质量，以保障核心服务的稳定运行。通过降级操作，可以确保在系统资源不足时，关键服务如交易系统能够继续正常运作。这是一个丢车保帅的操作，实施服务降级需要综合考虑和周密规划，以确保在面临不同的系统挑战时能够灵活应对。

#### 关键考虑因素

- **核心服务识别**：明确哪些服务是系统的核心，必须保持不间断运行的。
- **降级触发条件**：设定自动或手动触发降级的条件，如流量峰值、错误率升高等。
- **降级策略**：根据不同的服务和场景，制定具体的降级策略，如关闭非核心功能、降低服务质量等。
- **恢复机制**：定义服务从降级状态恢复到正常状态的条件和操作流程。

#### 实施举措

服务降级的技术实现可以多种多样，主要包括但不限于：

- **功能切换**：临时关闭或限制某些非核心功能，如搜索、推荐等。
- **请求拒绝**：当流量超过预设阈值时，直接拒绝部分请求，特别是非关键请求。
- **资源隔离**：对不同的服务进行资源隔离，确保关键服务有足够资源。
- **异步处理**：将同步处理的请求改为异步，减轻即时压力。

服务降级是保证系统高可用性的重要策略之一，正确实施降级策略可以在关键时刻保护系统的核心功能，减少服务中断对用户和业务的影响。通过全面的考虑和精心的规划，即使在资源有限的情况下，也能确保系统的稳定性和可用性。



### 超时与重试机制

#### 代理层超时与重试

##### Nginx 超时设置概览

在Nginx配置中，超时设置是保障服务稳定性和响应性的关键参数。它们主要分为四大类：客户端超时、DNS解析超时、代理超时，以及在使用ngx_lua模块时的Lua相关超时。合理配置这些超时参数，可以优化客户端与服务器之间的交互，防止长时间占用服务资源。

###### **客户端超时设置**

客户端超时设置通常有读取请求头超时时间、读取请求体超时时间、发送响应超时时间、长连接超时时间

- **client_header_timeout:** 控制服务器读取客户端请求头的最大等待时间。默认60秒，超时后返回408(Request Time-out)状态码。

- **client_body_timeout:** 规定服务器读取客户端请求体的最大等待时间，默认60秒。用于定义服务器在接收客户端POST或PUT请求数据时，两次读操作之间的最大间隔。超时未完成读取，则返回408状态码。

- **send_timeout:** 设置服务器向客户端发送响应数据的超时时间，默认60秒。此参数定义从服务器开始发送响应到下一次发送数据包的最大时间间隔，超时会中断连接。

- **keepalive_timeout [timeout] [header_timeout]:** 确定HTTP长连接的保持时间。通过减少每次请求/响应的TCP连接建立的开销，长连接提升了性能。
  
  - **`timeout`**: 指定TCP连接在空闲时保持打开状态的最长时间，超过此时间后无数据交换连接将被关闭，默认值通常为75秒。此参数直接影响服务器如何管理长连接。通过限制长连接的持续时间，服务器可以更有效地管理其资源，避免过多的空闲连接消耗内存和处理能力。
  
  - **`header_timeout`**（可选）: 设置响应头中`Keep-Alive`超时值，作为客户端保持连接打开的提示。此值为客户端提供服务器希望保持连接的时长的信息，而不直接影响服务器的连接处理策略。

###### DNS解析超时设置

在Nginx配置中，`resolver_timeout`指令用于设置DNS解析的超时时间，默认值为30秒。这个设置是在Nginx需要解析域名时尤其重要，比如在代理请求或配置上游服务器时使用域名而非直接的IP地址。为了进行有效的DNS域名解析，`resolver_timeout`需与`resolver`指令配合使用。

- **resolver**: 指定DNS服务器的地址，Nginx会使用这些服务器进行域名解析。该指令还可以接受一个`valid=time`参数，用于指定解析结果的有效期。

- **resolver_timeout**: 控制DNS解析操作的超时时间。如果在设定的时间内DNS解析未完成，Nginx将停止解析过程并返回错误。适当配置此值可以防止因DNS解析延迟导致的请求处理延迟。

```nginx
http {
    # 指定DNS服务器地址和域名解析结果的有效期
    resolver 192.0.2.1 valid=300s;
    
    # 设置DNS解析超时时间为30秒
    resolver_timeout 30s;
    ...
}
```

###### 代理超时设置

主要有三组配置：网络连接/读/写超时设置、失败重试机制设置、upstream存活超时设置。

- 网络连接、读取、发送超时设置
  
  - **proxy_connect_timeout**: 定义与上游服务器建立连接的超时时间，默认60秒。适当调整此值可以防止因连接延迟导致的请求阻塞。
  
  - **proxy_read_timeout**: 指定从上游服务器读取响应数据的超时时间，默认60秒。此参数确保Nginx能够及时放弃长时间未响应的连接。
  
  - **proxy_send_timeout**: 设置向上游服务器发送请求的超时时间，默认60秒。调整此值有助于处理上游服务器接收能力较弱的情况。

- 失败重试机制
  
  - **proxy_next_upstream [error | timeout | invalid_header | http_xxx | non_idempotent | off]:** 配置在什么情况下需要进行重试，`http_xxx`允许重试特定的HTTP状态码返回的请求，`off`表示禁用重试
  
  - **proxy_next_upstream_tries**: 控制Nginx重试上游服务器的最大次数，0表示不限制。
  
  - **proxy_next_upstream_timeout**: 设置在给定时间内（如30秒），Nginx尝试所有可用上游服务器的总时长，0表示不限制时间。

- upstream存活设置
  
  - **max_fails**: 指定在`fail_timeout`时间段内允许请求失败的最大次数，超过该次数后，上游服务器将被认为是不可用的。
  
  - **fail_timeout**: 定义在此时间段内`max_fails`次失败后，服务器被标记为不可用的时间。**

- ngx_lua超时设置

示例如下

```nginx
upstream backend_server {
    server 192.x.x.x:xxxx max_fails=2 fail_timeout=10s weight=1;
    server 192.x.x.x:xxxx max_fails=2 fail_timeout=10s weight=1;
}

server {
    ...
    location / {
        proxy_connect_timeout 5s;
        proxy_read_timeout 5s;
        proxy_send_timeout 5s;

        proxy_next_upstream error timeout http_500;
        proxy_next_upstream_timeout 0;
        proxy_next_upstream_tries 0;

        proxy_pass http://backend_server;
        add_header upstream_addr $upstream_addr;
    }
}
```

     
