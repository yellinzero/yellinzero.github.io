---
title: Architecture Learning(WIP)
date: 2024-03-08 14:55:28
tags:
    - architecture
categories: 
    - general
lang: zh-CN
---

# 前言

在过去，我曾将架构视为一门纯粹的技术领域。然而，随着我亲自深入服务器管理和服务搭建的实践，我逐渐领悟到架构其实是一套旨在解决关键问题的方法论——即**如何在资源有限的条件下寻找最优解**。这让我认识到，我所学习的一切，都应该围绕这一核心问题展开。

架构的思考可以从两个主要方面入手：

- **评估：** 对项目进行全面分析，确定完成目标所需的资源量，以及我们能够调动的资源量。在现有资源限制下，是否有可能实现目标？能否用更少的资源达到同样的效果？

- **设计：** 评估完成后，基于可用资源进行架构设计，这可能涉及到服务器资源配置、研发资源分配、团队协作、技术实现选型等多个层面。

作为架构师，不可忽视评估与设计这两个环节。尤其是评估阶段，它经常被忽略。有时候，我们可能轻易地认为某个目标不可实现，但应反复探索是否存在其他路径，通过更少的资源或在可接受的牺牲条件下实现目标。

我倾向于从更宏观的角度去思考架构问题。因为我们面临的挑战不仅仅是技术问题。从项目构建到团队合作、从技术选型到项目管理，我希望能够培养出一种思维方式，而不仅仅是掌握具体的实现工具。

接下来，我将基于《亿级流量：网站架构核心技术》一书，深化我的架构知识。我会结合书中内容和个人思考记录笔记。这本书是一位前领导推荐的架构入门读物，我期待它能帮助我达到新的高度。

# 原则

## 墨菲定律、康威定律和二八定律

在构建系统架构时，吸纳和运用跨领域内的核心原则不仅能提高系统的健壮性、效率和可维护性，而且能够指引我们避免常见陷阱。三个广泛认知的原则——墨菲定律、康威定律和二八定律来为我们提供通用的策略和思路

### 墨菲定律

墨菲定律的精髓在于 **“凡是可能出错的事，总会出错”** 。这个看似悲观的原则其实是一种强大的风险管理工具，提醒我们在设计时预见最坏的情况，并提前准备应对策略。

**应用策略：**

- **设计冗余**: 系统设计时考虑到可能的失败点，并提前设计好备份或冗余机制，以保证系统在组件失败时能够持续运作。
- **容错机制**: 实现容错机制，如异常处理、事务回滚等，确保系统在面临意外情况时能够优雅地恢复。
- **灾难恢复计划**: 制定和测试灾难恢复计划，确保在极端情况下能够迅速恢复服务。

### 康威定律

康威定律：**“系统设计（架构）会被限制于创建这些系统的组织结构。”** 这意味着组织的沟通结构将直接影响到最终系统设计的形状。这揭示了系统设计与创造它的组织结构之间的内在联系，强调了沟通结构对架构成果的影响。

**应用策略：**

- **组织结构调整**: 根据所需的架构目标，调整团队结构，以促进更有效的沟通和合作，确保架构设计能够顺利实施。
- **跨部门合作**: 鼓励跨职能团队的合作，打破部门间的壁垒，促进更灵活和创新的系统设计。
- **架构一致性**: 维护架构决策的一致性和清晰性，确保各团队朝着共同的目标努力，减少架构腐化的风险。

### 二八定律（帕累托原则）

帕累托原则指出 **“80%的效果来自20%的原因”** ，提示我们识别并集中资源于那些最具影响力的领域。这就跟《毛选》里面说的一样，我们应该集中最大的力量去解决更紧迫更具体的需求，这样才能更好更快的达成目标。

**应用策略：**

- **性能优化**：识别并优化那20%可能造成80%性能瓶颈的代码或组件。
- **功能开发**：优先开发那20%对用户最重要的功能，以提高用户满意度和业务价值。
- **问题解决**：针对导致大多数问题的根本原因进行解决，而非仅仅修复表面症状。

每个原则都提供了一种视角，它们帮助我们在复杂的系统设计过程中，做出更加明智的决策。通过对这三个原则的深入理解与应用，我们可以更有效地规避潜在的风险，减少团队的人效困境，同时提高系统的性能和用户满意度。

## 高并发原则

### 为什么无状态设计容易进行水平扩展？

- **无需维护客户端状态**: 由于每个请求都是自包含的，服务器不需要跟踪或维护客户端的状态信息，这简化了数据管理和同步的需求。

- **负载均衡的简化**: 无状态应用程序更容易实现负载均衡。请求可以被任意地分配到集群中的任何服务器上，而不需要担心用户状态和数据的连续性问题。

- **弹性和可伸缩性**: 无状态架构允许系统根据需要动态添加或移除资源。这意味着应用可以根据需求轻松扩展（水平扩展）或缩减，提高资源利用效率和成本效益。

- **容错性和恢复能力**: 在无状态设计中，如果某个服务器失败，其他服务器可以无缝地接管请求，因为没有必要访问存储在故障服务器上的状态信息。

> **示例** 
> 
> 一个典型的无状态设计示例是基于HTTP协议的Web应用。HTTP本身是一个无状态协议，意味着每个请求都是独立的，不依赖于之前的请求。在这种设计下，Web服务器不会保存任何用户的状态信息（如登录状态、浏览历史等）。这种无状态的Web应用可以非常容易地通过增加更多的Web服务器来实现水平扩展，每个服务器都可以独立处理进来的请求。
> 
> 例如，一个电子商务平台的产品目录服务可以设计成无状态的。当用户浏览产品时，每个请求都包含了获取特定产品信息所需的全部信息（如产品ID）。服务器处理这个请求，返回产品的详细信息，而不需要知道用户之前的浏览历史。这样，该服务就可以通过增加更多的服务器实例来轻松扩展，每个实例都能独立响应用户请求，提高系统的吞吐量和可用性。

### 服务化：逐步深化以满足需求

服务化是指将应用程序的不同功能模块化并作为独立服务部署的过程，它显著提升了软件的可维护性、可扩展性和复用性。随着服务化程度的增加，我们见证了从简单结构到复杂系统、从初级优化到高级治理的转变。

#### 进程内服务：基础模块化

- **概念**：功能模块以库或类的形式存在于单一应用进程中，通过直接方法调用实现服务。
- **示例**：一个Web应用中，用户认证和数据库访问等功能以不同的类或库形式实现，并在同一进程中运行。此方法简单直接，但在灵活性和扩展性方面有所不足。

#### 单机远程服务：初步独立

- **概念**：功能模块作为独立服务在单服务器上运行，通过网络协议（如HTTP、RPC）进行远程调用。
- **示例**：用户认证功能被拆分为独立服务，在同一台服务器的不同进程中运行，并通过HTTP API进行服务调用。这提高了模块独立性，但仍限于单机资源。

#### 集群手动注册服务：提升可用性

- **概念**：服务部署于多机集群，地址和端口通过中心或配置文件手动注册，以便消费者发现和调用。
- **示例**：用户认证服务在多机集群中运行，服务地址通过共享配置文件手动配置。客户端根据配置发现并调用服务，增强了服务的可用性和负载处理能力，但增加了管理复杂度。

#### 自动注册与发现服务：简化管理

- **概念**：服务启动时自动注册到服务注册中心，消费者通过查询注册中心自动发现服务地址。
- **示例**：用户认证服务启动时，自动注册其地址到如Consul或Eureka的服务注册中心。客户端应用通过查询注册中心自动获得服务地址，极大简化了服务管理和发现过程。

#### 服务的分组/隔离/路由：增加灵活性

- **概念**：基于服务版本、租户或其他属性进行服务分组或隔离，并根据请求特征智能路由。
- **示例**：用户认证服务可按版本或用户群体隔离，并通过API网关根据请求的版本号或用户属性进行路由。这提升了服务灵活性和可维护性。

#### 服务治理：确保高可用性

- **概念**：包括监控、容错、负载均衡、限流、熔断等机制，确保服务高可用性和稳定性。
- **示例**：对用户认证服务实施服务治理，实现服务健康监控、自动扩缩容、请求限流和熔断机制等。使用如Netflix Hystrix的框架支持这些功能。

通过以上递进介绍，服务架构从简单的进程内调用发展至可自管理和治理的分布式服务网络。每个阶段都增强了服务的独立性、扩展性和可靠性，同时也带来了更高的管理复杂度。设计服务架构时，应根据具体需求和资源状况，权衡不同服务化级别的适用性。

### 消息队列：服务解耦、异步处理与性能优化

消息队列是现代软件架构中不可或缺的一部分，提供了一种高效的机制来解耦服务、实现异步处理，以及在高负载环境下平滑流量。此外，对于性能优化和故障容错方面，消息队列也发挥着重要作用。

#### 服务解耦

- **核心思想**：通过消息队列，生产者仅需将消息发送到队列，无需关心消息的具体消费者，实现了生产者与消费者的解耦。

- **应用示例**：在电商系统中，订单服务发布订单信息至消息队列，而库存和支付服务则作为独立的消费者处理相关任务，实现了服务之间的解耦。

#### 异步处理

- **核心思想**：消息队列支持将非即时的任务异步化处理，提升系统的响应速度和用户体验。

- **应用示例**：用户注册流程中，发送欢迎邮件和生成推荐列表等后续操作通过消息队列异步执行，避免阻塞主流程。

#### 流量削峰/缓冲

- **核心思想**：在流量高峰期，消息队列作为缓冲层，帮助平滑处理请求，防止后端服务过载。

- **应用示例**：秒杀活动中，购买请求首先进入消息队列，根据后端服务能力逐步处理，有效防止服务崩溃。

#### 性能优化

- **核心思想**：通过扩展消息队列的实例来分散负载，提升系统整体吞吐量。

- **应用示例**：大型活动中，部署多个消息队列副本分担消息流，每个实例处理部分消息，提高处理效率。

#### 防重和错误处理

- **核心思想**：实现消息的去重和错误处理机制，确保消息处理的准确性和可靠性。

- **应用示例**：
  
  - **防重**：利用订单ID等唯一标识符确保消息仅被处理一次。
  - **错误处理**：设置重试策略或将失败消息转入“死信队列”以便后续处理。

#### 消息队列带来的挑战

尽管消息队列带来许多优势，但其引入也增加了系统的复杂性。主要挑战包括： 

- **架构和维护复杂度**：引入消息队列意味着需要额外管理和维护此组件，包括监控、故障排除等。
- **并发和异步处理的复杂性**：高并发下的消息有序处理、异步通信中的稳定性保障等。
- **事务性和数据一致性**：分布式系统中，保持数据一致性和管理跨服务事务的复杂性。
- **重复和丢失消息处理**：设计机制处理消息的重复投递和潜在的消息丢失问题。

通过精心设计和选择适合的消息队列产品（如[RabbitMQ](https://www.rabbitmq.com/)、[Apache Kafka](https://kafka.apache.org/)、[Amazon SQS](https://aws.amazon.com/cn/sqs/)等），可以有效地克服这些挑战，充分发挥消息队列在现代分布式系统中的价值。

### 数据异构

在现代软件开发和大数据处理领域，数据异构和数据闭环的概念是解决多源数据集成、管理和应用中的核心问题。在书中也提到的这两个概念，主要强调了如何处理来自多个数据源的数据问题，以及如何精确、稳定地处理和使用这些数据。在实际应用场景中，我们常常需要从多个服务收集数据，这些服务返回的数据可能结构不同，而且我们往往只需要每个数据源的部分数据。这不仅是一个关于数据处理的问题，更涉及到数据的整合、准确性和系统稳定性的维护。

我们常常会面临以下的挑战：

- **数据结构不一致**：不同来源的数据可能采用不同的格式和结构，如何有效地将它们整合成一致的格式是一个主要挑战。
- **数据质量和准确性**：如何确保聚合后的数据既准确又可靠，对于维护系统稳定性至关重要。
- **系统可维护性**：随着数据源的增加和数据量的膨胀，如何保持系统的可维护性和扩展性成为一项挑战。

我们可以通过下面的策略来解决：

- **中间层聚合**：在后端或中间层实施数据聚合逻辑，将来自不同源的数据标准化并聚合成统一格式，再提供给前端消费。这种方法有助于简化前端逻辑，减少前端处理数据的复杂度。
- **ETL流程**：通过ETL（提取、转换、加载）工具和流程，对数据进行提取、清洗、转换和加载，确保数据的一致性和准确性。ETL过程是大数据处理中常用的技术，有助于处理和整合异构数据源。

前端一样会面对数据处理的问题，我们通常会通过状态管理工具（如Redux、Pinia）和代码逻辑来保证数据同步与一致性问题。通常，我个人还是更推荐把复杂数据的处理放到后端，前端更多的把注意力集中在数据的使用上。

### 缓存技术：加速应用性能和降低延迟

缓存技术通常是优化应用性能、减少延迟和降低对原始数据源访问压力的关键方法。它通过暂存数据的副本，避免了重复的数据请求，从而加快数据检索速度并提升用户体验。下面是各种缓存策略及其实际应用场景的简单介绍。

#### 浏览器端缓存

- **核心**：缓存网页的静态资源，如HTML、CSS、JavaScript和图像，减少服务器请求。
- **实践**：利用HTTP头部`Cache-Control`和`Expires`指令控制资源缓存。例如，`Cache-Control: max-age=3600`告诉浏览器一个小时内不需重新请求该CSS文件。

#### 客户端缓存

- **核心**：移动或桌面应用内缓存数据，以减少网络请求并提速，另外还可以提供离线的客户端操作作为兜底方案，比如缓存首屏资源。
- **实践**：在大型促销活动前，将静态资源预加载到客户端，减轻活动启动时服务器的负担。这通常涉及资源版本控制和预加载等技术。

#### [CDN](https://zh.wikipedia.org/zh-cn/%E5%85%A7%E5%AE%B9%E5%82%B3%E9%81%9E%E7%B6%B2%E8%B7%AF)缓存

- **核心**：内容分发网络（CDN）在全球多地缓存内容，提高全球访问速度。
- **实践**：将静态资源部署至CDN，实现资源请求由用户最近的服务器响应，显著减少加载时间。

#### 接入层缓存

- **核心**：在架构的前端部署缓存，存储频繁访问内容。
- **实践**：
  - **配置[Nginx](https://nginx.org/en/)作为反向代理缓存**：设置Nginx来缓存动态内容的输出。例如，可以对经常访问的首页、产品列表页等进行缓存。通过Nginx的`proxy_cache`指令，你可以定义缓存的键、有效期和存储路径。
  - **使用[Varnish](https://github.com/varnishcache/varnish-cache)进行高级缓存**：Varnish是另一个强大的HTTP缓存系统，能够处理更复杂的缓存逻辑。通过VCL（Varnish Configuration Language），你可以编写规则来决定哪些请求和响应应该被缓存以及如何缓存。

#### 应用层缓存

- **核心**：应用内部缓存机制，针对特定数据或计算结果进行缓存。

- **实践**：
  
  - **内存缓存**：使用[Redis](https://redis.io/)或[Memcached](https://memcached.org/)缓存数据库查询结果或其他计算数据。
  - **应用内缓存机制**：在代码级别实现缓存，如利用HashMap缓存计算密集型数据结果。

#### 分布式缓存

- **核心**：构建分布式缓存系统，保障大规模应用的数据可扩展性和高可用性。

- **实践**：
  
  - **部署Redis或Memcached集群**：实现数据的自动分片和复制。
  - **数据一致性和分片策略**：采用一致性哈希算法等技术确保缓存系统的负载均衡和高可用。

正确利用各类缓存技术不仅能提高应用的性能，还能在高流量情况下保持系统的稳定性，显著改善最终用户的体验。每种缓存策略都有其适用场景和考虑要点，根据应用的具体需求和环境选择最合适的缓存解决方案至关重要。

除了在整体架构中采用缓存方案外，缓存技术在前端应用开发中也扮演着至关重要的角色，尤其是在提升用户交互体验方面。以实时编辑器为例，利用本地缓存暂存用户输入的内容可以显著改善在网络条件不佳时的使用体验。当用户的网络连接恢复时，应用可以在后台自动保存并同步数据，从而确保用户在编辑过程中几乎感受不到任何延迟或卡顿。这种方法不仅保证了数据的安全性和实时性，也使用户体验更加流畅和愉悦。

自然，任何缓存策略都会遇到数据一致性和有效性的挑战。因此，设计一个高效且运行良好的缓存方案变得至关重要。这不仅涉及到技术选择和实现细节，还包括对数据更新机制、失效策略和一致性保障的周到规划，以确保缓存带来的性能提升不会以牺牲数据准确性为代价。

## 高可用原则

### 系统降级策略

在管理高流量的网站架构时，为了保持系统的高可用性和稳定性，采用降级策略是一种常见做法。这些策略使我们能够在面对极端流量或系统压力时，灵活应对，确保关键服务的持续可用。下面是几种典型的降级策略：

#### 开关集中化管理

集中化管理系统的各类开关（如功能开关、流量控制开关）能够使我们在系统状态发生变化时迅速作出反应，比如临时关闭某些功能或服务以应对突增的流量。

**示例**：想象在大促销期间，电商平台的某个后端服务（例如，推荐系统）面临极大的访问压力。通过集中管理的开关，运维团队可以迅速禁用推荐服务，减轻系统负荷，而不会影响到平台的整体运行。

#### 可降级的多级读服务

在一个拥有多级缓存架构的系统中，数据会被存储在不同的级别（例如，内存、本地缓存、分布式缓存）。可降级的多级读服务允许系统在上级缓存不可用时自动降级到下一级缓存或直接从数据库中读取数据，保障数据的持续可用性。

**示例**：在一个既使用了内存缓存也使用了分布式缓存的系统中，如果内存缓存由于某些原因（如服务器重启）不可用，系统将自动切换到分布式缓存来获取数据。如果分布式缓存同样无法访问，系统则直接从数据库中读取数据，确保服务的连续性。

#### 开关前置化

将流量控制和功能开关等控制逻辑放置于系统的最前端（如负载均衡器或API网关），这样可以在请求进入应用层之前进行必要的流量控制和功能调整，减少后端服务的负担。

**示例**：在Nginx+Tomcat的架构下，若Tomcat应用承受过大的流量压力，Nginx层可以通过流量限制或重定向某些请求到错误或降级页面，有效阻止过量流量影响Tomcat层。

##### 实现Nginx层的开关控制策略

- **流量限制（Rate Limiting）**：利用Nginx的限流指令（如`limit_req_zone`），可以基于每秒请求数量设定限流规则，对超出限制的请求进行延迟处理或返回错误响应，有效避免后端服务过载。

- **功能开关（Feature Toggle）**：通过Nginx配置中的`if`语句或逻辑判断，根据请求的URL、时间等条件动态启用或禁用特定功能，灵活控制服务的可用性。

- **维护模式**：在系统需进行维护或升级时，可快速在Nginx配置中启用维护模式，将所有请求引导至维护页面，而无需中断后端应用运行。

- **示例配置：**
  
  ```nginx
  # 指令用于定义一个限流区域，这个区域将用于存储会话状态。
  # https://nginx.org/en/docs/http/ngx_http_limit_req_module.html#limit_req
  # $binary_remote_addr 表示限流是基于每个独立IP进行的。
  # zone=mylimit:10m 创建一个名为 mylimit 的存储区域，大小为 10MB。这个存储大小决定了 Nginx 能够跟踪的并发请求数量。
  # rate=10r/s 设定允许通过的请求速率为每秒 10 个请求
  limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;
  
  server {
      listen 80;
      server_name example.com;
  
      # 维护模式开关
      set $maintenance 0; # 0 表示正常运行，1 表示维护模式
  
      if ($maintenance) {
          return 503 '<html><body>系统维护中，请稍后再试。</body></html>';
      }
  
      location / {
          # 流量限制配置
          limit_req zone=mylimit burst=20 nodelay;
  
          # 正常情况下的请求转发
          proxy_pass http://tomcat_backend;
      }
  
      # 定义一个特殊的URI用于返回"系统繁忙"页面
      location /busy {
          return 503 '<html><body>系统繁忙，请稍后再试。</body></html>';
      }
  
      # 这部分被注释的配置表示一个降级操作的示例。
      # 若取消注释，所有对根路径 / 的请求将被重写到 /busy
      # 导致所有用户都会看到“系统繁忙”的页面。
      # 这是一种简单有效的流量控制手段，用于紧急情况下快速减轻后端系统压力。
      # 当系统需要降级处理时，将所有请求重定向到/busy
      # 通过修改配置文件和重新加载Nginx来动态开启降级
      # location / {
      #     rewrite ^ /busy;
      # }
  }
  ```

我们再具体说一下nginx限流区域的问题，这时nginx通过`limit_req_zone`指令，用于跟踪和控制请求的速率。这个区域在Nginx的工作进程之间是共享的，所以限流规则可以跨多个客户端连接和请求生效，这里面存放的主要是请求的计数器或标记，它们主要用于跟踪特定的键值（如客户端的IP地址），通常不需要太大的内存。

#### 业务降级

当系统压力过大或某些服务不可用时，暂时关闭或降低某些非核心业务的服务质量，从而确保系统的核心功能能够稳定运行。

**示例**：在电商网站的大促活动期间，为了保障订单系统的平稳运行，可能会暂时关闭或降级一些辅助功能，如商品推荐、评论功能等，以优先保障订单处理等核心业务的资源需求。

### 限流、流量切换和版本控制

- 限流是预防系统过载的重要手段，尤其针对突发或恶意流量。上面提到的Nginx的限流配置提供了一个实际操作的例子，通过控制请求速率来避免服务崩溃。

- 流量切换能够在服务出现故障时迅速响应，减少服务中断时间。下面是一些常见的方式：
  
  - **DNS切换**：通过更改DNS记录，切换到备用机房或服务入口，实现大范围的流量重定向。
  
  - **[HttpDNS](https://zhuanlan.zhihu.com/p/102839806)**：在应用程序场景下，HttpDNS允许客户端直接确定流量入口，绕过可能存在问题的本地DNS，实现更精细的流量控制。
  
  - **[LVS/HaProxy](https://zhuanlan.zhihu.com/p/71825940)**：用于在负载均衡层面切换流量，以隔离或绕过出现故障的Nginx接入层。
  
  - **Nginx切换**：在应用层面进行切换，如将流量从故障服务重定向到健康服务，保持应用的可用性。

- 良好的版本控制不仅保证了代码的可审计性和可回溯性，也支持快速回滚到稳定版本以应对新版本可能引入的问题。有效的版本管理策略和回滚机制对于快速定位问题和恢复服务至关重要。
  
  - **可审计**：确保每次发布都有明确的版本记录，便于追踪变更和定位问题。
  
  - **可回溯**：在发现新版本问题时，能够迅速回滚到上一个稳定版本，减少故障影响时间。
  
  - **回滚机制**：设计灵活可靠的回滚操作，包括自动化脚本和手动干预，以应对各种回滚场景。

# 高可用策略

## 负载均衡与反向代理

负载均衡，简单点说，就是会将用户的访问分配到不同的服务器上，以解决单服务器压力过大或是服务器出现故障等问题。外网DNS使用全局负载均衡（GSLB）来进行流量调度，内网则通过简单的轮询负载均衡或借助一些专用的中间层，如LVS、F5、HaProxy、Nginx等。对于一般应用而言用Nginx即可，它一般用于HTTP七层负载均衡，现在也已支持TCP四层负载均衡。我们在简单解释下上面提到的几个概念：

- 全局负载均衡：全局负载均衡（Global Load Balancing）是一种网络架构或技术，旨在平衡流量和请求分发到多个地理位置的服务器或数据中心。

- 二层负载均衡：是指在 OSI 模型的数据链路层（第二层）对网络流量进行负载均衡的一种技术。这通常涉及到对数据包的目标 MAC 地址进行改写，将其修改为负载均衡器与真实服务器之间的 MAC 地址。这样做的目的是让负载均衡器能够接收到流量，然后再根据负载均衡策略将流量转发给相应的真实服务器。
  
  **数据链路层的负载均衡**：
  
  - 在二层负载均衡中，负载均衡器工作在 OSI 模型的数据链路层。它通过修改数据包的目标 MAC 地址来拦截并重定向流量。
  - 负载均衡器和真实服务器共享同一个虚拟 IP 地址（VIP），因此所有传入的数据包都会被发送到负载均衡器。
  - 当负载均衡器接收到数据包后，它会根据负载均衡策略选择一个真实服务器，并将数据包的目标 MAC 地址修改为该服务器的 MAC 地址，然后将数据包发送到网络上。
  
  **改写数据包的 MAC 地址**：
  
  - 负载均衡器通过改写数据包的目标 MAC 地址来实现流量的转发。这样做的效果是，流量似乎直接发送给了真实服务器，而不是经过负载均衡器。
  - 负载均衡器与真实服务器之间需要事先建立好 ARP（地址解析协议）缓存，以确保负载均衡器能够正确识别真实服务器的 MAC 地址。
  
  **VIP 的作用**：
  
  - 虚拟 IP 地址（VIP）是负载均衡器和外部客户端通信的入口。所有传入的流量都会发送到 VIP 上。
  - 通过共享同一个 VIP，负载均衡器能够接收到所有传入的流量，并根据负载均衡策略将流量转发给真实服务器。
  
  **LVS DR 工作模式**：
  
  - LVS（Linux Virtual Server）是一个开源的负载均衡器软件，支持多种负载均衡模式，包括 Direct Routing（DR）模式。
  - 在 LVS DR 模式中，负载均衡器与真实服务器在同一个局域网内，它们共享同一个 VIP，并且负载均衡器不会修改数据包的 IP 地址。只有目标 MAC 地址会被修改为真实服务器的 MAC 地址，以实现流量的转发。

- 四层负载均衡：是指在 OSI 模型的传输层（第四层）对网络流量进行负载均衡的一种技术。这种负载均衡是根据传输层的信息，如源 IP 地址、目标 IP 地址、源端口和目标端口等，来决定如何转发数据包。四层负载均衡通常用于基于端口的负载均衡，它能够将传入的连接或数据包根据端口信息分发到不同的上游服务器。如LVS NAT模式、HaProxy等

- 七层负载均衡：是指在 OSI 模型的应用层（第七层）对网络流量进行负载均衡的一种技术。这种负载均衡是根据应用层的信息，如主机名、URL、HTTP 头部等，来决定如何转发数据包。七层负载均衡通常用于基于应用层协议的负载均衡，例如 HTTP、HTTPS、SMTP、FTP 等。常见软件如HaProxy、Nginx等

因为LVS和上游服务器必须在同一个子网，为了解决跨子网问题而又不影响负载性能，我们可以使用LVS+HaProxy(or Nginx)的集群来解决跨网和性能问题。

对于负载均衡我们通常要关心以下的内容

- 上游服务器的配置

- 负载均衡的算法

- 失败重试机制

- 服务器心跳检查

通常我们会搭配各种软件来进行更智能的负载均衡，比如OpenResty。另外Nginx除了是负载均衡还是一台[反向代理](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86)服务器。

### upstream配置

```nginx
# 定义后端服务器，weight为此服务器的权重
upstream backend {
    server 192.168.61.1:9080 weight=1;
    server 192.168.61.1:8080 weight=2;
}

location / {
    proxy_pass http://backend;
}
```

### 负载均衡算法
